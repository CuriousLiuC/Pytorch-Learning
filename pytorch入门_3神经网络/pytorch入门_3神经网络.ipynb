{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch入门_3神经网络.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKuB3kAS-5d2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#3. 神经网络\n",
        "#pytorch中 torch.nn 专门用于实现神经网络，其中nn.Module包含了网络层的搭建，以及一个方法 -- forward(input)，并返回网络的输出output\n",
        "#实现一个经典的LeNet网络用来对字符进行分类\n",
        "\n",
        "#重点！神经网络的标准训练流程 多个部分\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "  #这个init函数的作用只是定义输入输出\n",
        "  def __init__(self):\n",
        "    super(Net,self).__init__()\n",
        "    #输入图像是单通道， 一层卷积核5x5大小，输出通道6\n",
        "    self.conv1 = nn.Conv2d(1, 6, 5)#输入1，输出6channel，卷积核是5x5大小的\n",
        "    #conv2，输入进6通道，输出16通道（也就是进行16次卷积操作），卷积核大小5x5\n",
        "    self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "    #全连接层\n",
        "    self.fc1 = nn.Linear(16*5*5, 120)\n",
        "    self.fc2 = nn.Linear(120,84)\n",
        "    self.fc3 = nn.Linear(84,10)#最后输出的是一个10通道的分类结果\n",
        "\n",
        "  def forward(self,x):#前向传播整个的流程怎么走\n",
        "    #max-pooling 采用一个2x2的滑动窗口\n",
        "    x = F.max_pool2d(F.relu(self.conv1(x)),2)\n",
        "    x = F.max_pool2d(F.relu(self.conv2(x)),2)\n",
        "    x = x.view(-1,self.num_flat_features(x))#转换成一个向量？ 16 *5 *5的\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        "  def num_flat_features(self,x): \n",
        "    #除了batch维度外的所有维度\n",
        "    size = x.size()[1:]# 这里为什么要使用[1:],是因为pytorch只接受批输入，也就是说一次性输入好几张图片，那么输入数据张量的维度自然上升到了4维。【1:】让我们把注意力放在后3维上面\n",
        "    num_features = 1\n",
        "    for s in size:\n",
        "      num_features *= s\n",
        "    return num_features #16 * 5 * 5 ↑\n",
        "net = Net()\n",
        "print(net)\n",
        "    \n",
        "#这里必须实现forward函数\n",
        "#在网络中，可以使用net.parameters()返回网络训练的参数（？）\n",
        "params = list(net.parameters())\n",
        "print(\"参数数量：\",len(params))\n",
        "# conv1.weight\n",
        "print(\"第一个参数大小：\", params[0].size())#这里可以一层一层的看\n",
        "\n",
        "#以下代码简单测试这个网络\n",
        "input = torch.randn(1,1,32,32)\n",
        "out = net(input)\n",
        "print(\"outtest:\", out) #最终输出10个\n",
        "\n",
        "\n",
        "#接着反向传播需要先清空梯度缓存，然后计算随机梯度进行反向传播\n",
        "#print(\"show : \",net.zero_grad())\n",
        "net.zero_grad()\n",
        "#print(\"show2 : \",torch.randn(1,10))\n",
        "out.backward(torch.randn(1,10))\n",
        "\n",
        "#nn.Conv2d接受的输入是一个4维张量，nSamples * nChannels * Height * Width"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwhLyg-JYsIJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "a6159f20-c1d4-46d4-f874-0a511b0d3f6a"
      },
      "source": [
        "#3.2损失函数loss func\n",
        "#损失函数的输入是(output, target) 网络输出和真是标签对的数据，返回一个数值表示网络输出和真实标签的差距\n",
        "#简单的损失函数是通过MSE均方误差来进行计算\n",
        "\n",
        "output = net(input)\n",
        "print(\"output:\",output)\n",
        "#定义伪标签,这里指的是这个target目标只是一个随便设置的，和网络输出无关，是一个randn\n",
        "target = torch.rand(10)\n",
        "print(\"target\",target)\n",
        "target = target.view(1,-1)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "loss = criterion(output,target)\n",
        "print(loss)#loss 也是一个1d的tensor"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "output: tensor([[-0.0270,  0.0323, -0.0253,  0.1146, -0.0260, -0.0662,  0.1373, -0.1628,\n",
            "         -0.0450, -0.0090]], grad_fn=<AddmmBackward>)\n",
            "target tensor([0.8017, 0.0283, 0.0251, 0.1488, 0.9078, 0.7367, 0.0570, 0.1321, 0.2928,\n",
            "        0.7309])\n",
            "tensor(0.2962, grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9s665dxXgM1W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "49e0e5cc-adc9-4839-ae71-ad308bb8924f"
      },
      "source": [
        "#如果调用loss.backward() 那么整个图是可微分的，也就是说包括loss，图中的所有张量是变量，只要是requires_grad = True， .grad张量都会随着梯度一直累积\n",
        "#MSELoss\n",
        "print(loss.grad_fn)\n",
        "#Linear Layer\n",
        "print(loss.grad_fn.next_functions[0][0])\n",
        "#Relu\n",
        "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<MseLossBackward object at 0x7fb3f5235320>\n",
            "<AddmmBackward object at 0x7fb3f5235b38>\n",
            "<AccumulateGrad object at 0x7fb3f5235320>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oInMhIOah6a2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "a8fdd006-7f1c-46b1-8e72-778b90826dbf"
      },
      "source": [
        "#3.3反向传播\n",
        "#反向传播只需要调用loss.backward()即可，首先要清空当前梯度缓存 .zero_grad()方法 否则造成梯度累加\n",
        "\n",
        "#conv1层的偏置参数bias反向传播前后结果\n",
        "net.zero_grad()\n",
        "print(\"conv1.bias.grad before backward\")\n",
        "print(net.conv1.bias.grad)\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "print('conv1.bias.grad after backward')\n",
        "print(net.conv1.bias.grad)\n",
        "\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv1.bias.grad before backward\n",
            "tensor([0., 0., 0., 0., 0., 0.])\n",
            "conv1.bias.grad before backward\n",
            "tensor([ 0.0120, -0.0025,  0.0159, -0.0193, -0.0040,  0.0111])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVaztZG1jQj9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#3.4更新权重\n",
        "#采用随机梯度下降，方法更新权重规则 SGD 随机更新权重？ 按照这个公式对权重进行随机的更新\n",
        "#weight = weight - learning_rate * gradient\n",
        "#简单实现更新权重的更新例子\n",
        "learning_rate = 0.01\n",
        "for f in net.parameters():\n",
        "  f.data.sub_(f.grad.data * learning_rate)\n",
        "\n",
        "#为了采用更多的优化算法，不仅仅是SGD，还有其他的算法，为了采用不同的方法，采用torch.optim 库，例子如下\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJZxxlg0li1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "#创建优化器 【optimizer是更新权重用的！！！！！！！】\n",
        "optimizer = optim.SGD(net.parameters(), lr = 0.01)\n",
        "\n",
        "#在训练过程中执行下列操作\n",
        "optimizer.zero_grad()#清空梯度缓存\n",
        "output = net(input)\n",
        "loss = criterion(output,target)\n",
        "loss.backward()\n",
        "optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}