{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch入门_2autograd.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "onjfsE5xsDy3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#2.autograd\n",
        "#对于python神经网络来说，autograd是一个非常关键的库，主要是提供对tensors上所有运算操作的自动微分，计算梯度功能\n",
        "#它属于define-by-run类型框架，即反向传播操作的定义是根据代码的运行方式，因此每次的迭代都可以是不同的\n",
        "\n",
        "#2.1张量\n",
        "'''\n",
        "  torch.Tensor是Pytorch中最主要的库，当设置其属性为.requires_grad = True, 那么就会开始追踪在该变量上的所有操作，而完成计算后，可以调用.backward()并自动计算所有梯度，得到的梯度都保存在属性.grad()中\n",
        "  上边这句话说得实在太抽象了，还要在实战中理解，总的来看是一个自动计算梯度用到的东西？\n",
        "\n",
        "  调用.detach()方法分离出计算的历史，可以停止一个tensor变量继续追踪其历史信息，同时也防止未来计算时候被追踪\n",
        "\n",
        "  …… 看理论看不懂，还实先从代码开始研究\n",
        "\n",
        "'''\n",
        "import torch\n",
        "\n",
        "x = torch.ones(2,2,requires_grad = True)#need gradient\n",
        "print(x)\n",
        "\n",
        "y = x + 2\n",
        "print(y)# 重点！ y是一个操作的结果，所以它带有属性grad_fn\n",
        "print(y.grad_fn)\n",
        "\n",
        "z = y * y * 3\n",
        "out = z.mean()\n",
        "print(\"z = \",z)\n",
        "print(\"out = \",out)\n",
        "\n",
        "#实际上，一个Tensor的变量默认requires_grad 是False， 可以像上边一样在定义时候指定属性为true，也可以在定义变量后 调用a.requires_grad_(True) 这里的_代表了会对原有的属性进行修改\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ktu85kJ05gfd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "efbebc4a-8709-4b31-bf73-771466be0857"
      },
      "source": [
        "a = torch.randn(2,2)\n",
        "a = ((a*3)/(a-1))\n",
        "print(a.requires_grad)\n",
        "a.requires_grad = True\n",
        "print(a.requires_grad)\n",
        "b = (a * a).sum()\n",
        "print(b.grad_fn)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "True\n",
            "<SumBackward0 object at 0x7f024a125518>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoWc_Kgq551n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "86febfbf-cd63-49fa-d91b-a4c7e1be97bf"
      },
      "source": [
        "#2.2 梯度，接下来就是计算梯度，进行反向传播操作。\n",
        "#上节所述，out变量定义是一个标量，因此out.backward() 相当于 out.backward(torch.tensor(1.)) \n",
        "#这句话也太难理解了吧\n",
        "\n",
        "out.backward()\n",
        "#输出梯度 d(out)/dx\n",
        "print(x.grad)\n",
        "\n",
        "#梯度公式还是不太熟练，但是这里能推导出4.5是因为这个初值是全1，也就是x = 1这类的"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[4.5000, 4.5000],\n",
            "        [4.5000, 4.5000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7V2YtwMX7cM4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "86c9f4f3-9338-4b77-9b38-c74d17a1cfad"
      },
      "source": [
        "#如果从数学上来说，有一个向量值函数，那么对应的梯度应该是一个jacobian matrix\n",
        "#一般来说，torch.autograd 就是用于计算雅克比向量乘积的工具，直接进入代码练习环节\n",
        "\n",
        "x = torch.randn(3,requires_grad=True) #[a,b,c]\n",
        "\n",
        "y  = x * 2\n",
        "while y.data.norm() < 1000: #直接说L2范数就懂了，距离也就是绝对长度\n",
        "  y = y * 2\n",
        "\n",
        "print(y)\n",
        "#y不是一个标量了，也不能直接计算完整的jabian行列式，但我们可以通过简单传递给向量backward()方法作为jacobian响亮的乘积，例子如下\n",
        "v = torch.tensor([0.1,1.0,0.0001],dtype = torch.float)\n",
        "y.backward(v)\n",
        "print(x.grad) #指的是相对于x的梯度？\n",
        "\n",
        "\n",
        "print(x.requires_grad)\n",
        "print((x**2).requires_grad)\n",
        "\n",
        "with torch.no_grad():#停止追踪变量历史进行自动梯度计算 这个还是很玄学，梯度计算应该是用在反向传播中的， 但就是怎么也理解不了这回事啊...\n",
        "  print((x**2).requires_grad)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-498.7531,  666.8201,  792.0768], grad_fn=<MulBackward0>)\n",
            "tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])\n",
            "True\n",
            "True\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}